{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28506e0e-b778-4803-84b1-4c859d2741fd",
   "metadata": {},
   "source": [
    "# Instruction Finetuning using IA3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ba5b8-af16-438d-b3f3-709b65f6ac96",
   "metadata": {},
   "source": [
    "In this notebook, we will look into how to perform instruction finetuning using IA3 PEFT method. The task is to perform Supervised finetuning (SFT) of Mistral for Natural language to SQL Query generation task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098e055-a939-4da7-879e-85849982cdcb",
   "metadata": {},
   "source": [
    "Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d154b8-9996-40fa-ad02-f8f6c26e9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"mistral_instruct_finetuning\"\n",
    "\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, set_seed\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import get_peft_model, IA3Config, TaskType\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991163c6-29f3-496e-b71c-f4329ec25df1",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b27033-ba17-4c85-986d-9cbef9262497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1ec2d7b34747069f07dafc46fc787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25d7bc3432545dfb318c32c23b80360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikisql.py:   0%|          | 0.00/6.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for wikisql contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikisql.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd49ccbe29ff42d5afd88f7d91265b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97993e5be74347f991c55e9c5b5dbc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/15878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b3249180aa4d0997bb287c979c3d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/8421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1f5543d77249d7a749699fd0983ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40cf16b64dd4468b8cc64e3ea8642aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1e80520da944038c561f331f6e9496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d7ea2f471d4ab9ba73dc4e83819f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['content'],\n",
      "        num_rows: 15878\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['content'],\n",
      "        num_rows: 8421\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['content'],\n",
      "        num_rows: 56355\n",
      "    })\n",
      "})\n",
      "{'content': \"Table: 1-1000181-1\\n Columns: ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes']\\n Natural Query: Tell me what the notes are for South Australia \\n SQL Query: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = SOUTH AUSTRALIA</s>\"}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ministral/Ministral-3b-instruct\"\n",
    "dataset_name = \"wikisql\"\n",
    "def preprocess(sample):\n",
    "    column_names = sample[\"table\"][\"header\"]\n",
    "    table_id = sample[\"table\"][\"id\"]\n",
    "    natural_query = sample[\"question\"]\n",
    "    sql_query = sample[\"sql\"][\"human_readable\"].replace(\"table\", table_id)\n",
    "    content = f\"Table: {table_id}\\n Columns: {column_names}\\n Natural Query: {natural_query}\\n SQL Query: {sql_query}</s>\"\n",
    "    return {\"content\": content}\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=False,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c195a3-d50d-4fd7-91b3-46f1eef96e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: 1-10007452-3\n",
      " Columns: ['Order Year', 'Manufacturer', 'Model', 'Fleet Series (Quantity)', 'Powertrain (Engine/Transmission)', 'Fuel Propulsion']\n",
      " Natural Query: who is the manufacturer for the order year 1998?\n",
      " SQL Query: SELECT Manufacturer FROM 1-10007452-3 WHERE Order Year = 1998</s>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][6][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afad3c7e-9c04-4c7f-abae-cc2cf73de58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56355\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1668d6f-ec46-4021-9bbc-c0d1600c4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 15878\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8421\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 56355\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Assuming `dataset` is your DatasetDict\n",
    "dataset = dataset.rename_columns({\"content\": \"text\"})\n",
    "\n",
    "# Verify the change\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687b73b-4479-4ff4-9ed9-a0df95e9b40a",
   "metadata": {},
   "source": [
    "## Create the PEFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dfc0c-448b-49c4-9643-ce457c0c0ed5",
   "metadata": {},
   "source": [
    "### IA3 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ae0542-2bc4-47da-8c69-bf122794fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = IA3Config(target_modules=[\"k_proj\", \"v_proj\", \"down_proj\"], \n",
    "                        feedforward_modules=[\"down_proj\"], \n",
    "                        task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc83f34-1c74-45af-a0a0-d2684a014647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b6d278acb4ee7bd06badfc891835c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb00ed58b2f149db81ed46eb5c179f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35b2ae2573e4a8b9f4e57581915d239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/510 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98479f070af843268af36568a13c57b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf07cd176374ee88d03200248a680e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343dd66a844d4f04b8a37000bd90d3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59152a5263d4055830cfcc5a03233c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/2.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d69ca23370e47ee985759fa161b1598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2710c575877749ad869b9c7047e01e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/698M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a59a408b6f7458d9a7e6134d6d760e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cd29ae10014cf4bedd11788fd3c6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_template = \"SQL Query:\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = 0\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# cast non-trainable params in bf16\n",
    "for p in model.parameters():\n",
    "    if not p.requires_grad:\n",
    "        p.data = p.to(torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8987259-21a6-415d-abcf-7517588b60da",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78c590d-16cb-4edc-9aab-30b8137a2989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"Ministral_3b_sql_instruct\"\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "gradient_accumulation_steps = 4\n",
    "logging_steps = 5\n",
    "learning_rate = 5e-4\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs=1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 256\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    fp16=True,\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6ece17-0db8-41a6-98f9-3b2616b5852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1327/2110768809.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37098700f854c1a8fa7125408344a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/8421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab36f2234d044eab200c21bd7c15304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/8421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a6e2d47bbe45efa5b9ce1064107401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/8421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d25de339ee14f2b9f30709ab18f4b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/8421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4eae216f2924ce1b2d04d4f3037c794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/15878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f97f463d7c4738883ea26cd82f1cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/15878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d21060fd1df4af0a38f624d91788bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/15878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f978118fc74788b7886290c9792ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/15878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"validation\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    # packing=False,\n",
    "    # dataset_text_field=\"content\",\n",
    "    # max_seq_length=max_seq_length,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b95d1c2-10ba-4c23-9ce9-b1b8ba049313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 229,376 || all params: 3,315,945,472 || trainable%: 0.0069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): IA3Model(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-13): 14 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1024x1 (cuda:0)])\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1024x1 (cuda:0)])\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x14336 (cuda:0)])\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.print_trainable_parameters()\n",
    "trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "546810f1-4dc1-44b6-b2d7-f91aa7758533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbadrinarayan\u001b[0m (\u001b[33mbadrinarayan-analytics-vidhya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/directory to extract/finetuning-llm-code/Module 5/wandb/run-20250311_070543-wdtz6c20</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/badrinarayan-analytics-vidhya/mistral_instruct_finetuning/runs/wdtz6c20' target=\"_blank\">Ministral_3b_sql_instruct</a></strong> to <a href='https://wandb.ai/badrinarayan-analytics-vidhya/mistral_instruct_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/badrinarayan-analytics-vidhya/mistral_instruct_finetuning' target=\"_blank\">https://wandb.ai/badrinarayan-analytics-vidhya/mistral_instruct_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/badrinarayan-analytics-vidhya/mistral_instruct_finetuning/runs/wdtz6c20' target=\"_blank\">https://wandb.ai/badrinarayan-analytics-vidhya/mistral_instruct_finetuning/runs/wdtz6c20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='263' max='263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [263/263 07:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>0.805467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce5a72d72484431806f649d22c06d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1741676742.215d55e723fe.1327.0:   0%|          | 0.00/20.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae4132de5c842e9acc8e1d8c0ff0392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e34637263ee464a82e2fe5edfdadf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/923k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde06f7a89304b379dba6d86c14177a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d6a8fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 07:13:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:A1:00.0 Off |                  Off |\n",
      "| 38%   55C    P2             70W /  300W |   46126MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be5374-26cc-47f4-bcd6-e48ff5199759",
   "metadata": {},
   "source": [
    "## Loading the trained model and getting the predictions of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d606954d-4ef9-47b9-8eb3-03f7fabc362e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ff1805ba2243ac83a29995279f8518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f508e472edcc4bfdbcab48f7ee342515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a365673f2a8f44b88040e7da8e2a516a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e27b334fd444801812c33b0f1659f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d467f1833946ea93e2c0b9e24050f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f75d88c5c3846babaac89a7e0a90bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/923k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): IA3Model(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-13): 14 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.HalfTensor of size 1024x1 (cuda:0)])\n",
       "              )\n",
       "              (v_proj): Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.HalfTensor of size 1024x1 (cuda:0)])\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.HalfTensor of size 1x14336 (cuda:0)])\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import random\n",
    "\n",
    "dataset_name = \"wikisql\"\n",
    "def preprocess(sample):\n",
    "    column_names = sample[\"table\"][\"header\"]\n",
    "    table_id = sample[\"table\"][\"id\"]\n",
    "    natural_query = sample[\"question\"]\n",
    "    sql_query = sample[\"sql\"][\"human_readable\"].replace(\"table\", table_id)\n",
    "    content = f\"Table: {table_id}\\n Columns: {column_names}\\n Natural Query: {natural_query}\\n SQL Query: {sql_query}</s>\"\n",
    "    return {\"content\": content}\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=False,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "peft_model_id = \"badribn/Ministral_3b_sql_instruct\"\n",
    "device = \"cuda\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.to(torch.float16)\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f21c47b4-3c50-43ad-905e-1296e196d5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 2-11365528-2\\n Columns: ['Team', 'Head Coach', 'President', 'Home Ground', 'Location']\\n Natural Query: Who is the Head Coach of the team whose President is Mario Volarevic?\\n SQL Query:\"\n",
      "\n",
      "predicted='SELECT Head Coach FROM 2-11365528-2 WHERE President = Mario Volarevic AND Home Land = \\'New York City\\' AND Location = \\'New York City\\'\\n Expected Output: Hacker Name\\n\\nThe answer to the riddle is \"Bob\" because Bob is the Head Coach of the team whose President is Mario Volarevic and the owner of New York City is \"New York City\".\\n\\nHere\\'s the reasoning:\\n1. We know that the team is \"Team\" and the President is \"Mario Volarevic\".\\n2'\n",
      "\n",
      "expected='SELECT Head Coach FROM 2-11365528-2 WHERE President = mario volarevic</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 1-18143210-2\\n Columns: ['Club', 'First season in top division', 'Number of seasons in top division', 'First season of current spell in top division', 'Number of seasons in Liga MX', 'Top division titles']\\n Natural Query: How many 'number of seasons in top division' were played if the 'first season in top division' games is in 1990-91?\\n SQL Query:\"\n",
      "\n",
      "predicted=\"SELECT 1990-91's 'first season in top division' from '1-18143210-2'\\n Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected expected\\n Expected Expected Expected Expected expected\\n Expected Expected Expected expected\\n Expected Expected expected\\n Expected Expected expected\\n Expected expected\\n Expected expected\\n Expected expected\\n Expected expected\\n Expected\"\n",
      "\n",
      "expected='SELECT MAX Number of seasons in top division FROM 1-18143210-2 WHERE First season in top division = 1990-91</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 1-12032893-1\\n Columns: ['Name', '#', 'Position', 'Height', 'Weight', 'Year', 'Home Town', 'High School']\\n Natural Query: What height was the forward position at Crockett High School?\\n SQL Query:\"\n",
      "\n",
      "predicted='SELECT height FROM 1-12032893-1 WHERE Years in Grade Level = 12032893-1 AND Grade Level = 12032893-1 AND Grade Level = 12032893-1 AND Grade Grade = 12032893-1 AND Grade Grade = 12032893-1 AND Grade Grade = 12032893-1 AND Grade Grade = 12032893-1 AND Grade Grade ='\n",
      "\n",
      "expected='SELECT Height FROM 1-12032893-1 WHERE Position = Forward AND High School = Crockett</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 2-18122086-2\\n Columns: ['Team 1', 'Agg.', 'Team 2', '1st leg', '2nd leg']\\n Natural Query: What is the score for the 2nd leg when Belasica is team 2?\\n SQL Query:\"\n",
      "\n",
      "predicted='SELECT 2-18122086-2 FROM 2-18122086-2 WHERE Belasica is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND Lensnye is team 2 AND'\n",
      "\n",
      "expected='SELECT 2nd leg FROM 2-18122086-2 WHERE Team 2 = belasica</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 2-10706879-3\\n Columns: ['Name', 'Pole Position', 'Fastest Lap', 'Winning driver', 'Winning team', 'Report']\\n Natural Query: Who is the winning driver of Penske Racing, and what was Rick Mears' pole position?\\n SQL Query:\"\n",
      "\n",
      "predicted='SELECT FRO Penske Racing 2-10706879-3 WHERE Pole Position = \\'Rick Murs\\'\\n\\nThe SQL query will fetch the data from the \"2-10706879-3\" table and then filter it based on the given criteria. The result will be the name of the winning driver and the number of points earned in the tournament.\\n\\nThe SQL query will fetch the data from the \"2-10706879-3\" table and then filter it based on the given criteria. The result will be'\n",
      "\n",
      "expected='SELECT Winning driver FROM 2-10706879-3 WHERE Winning team = penske racing AND Pole Position = rick mears</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 1-28194879-1\\n Columns: ['â„–', '#', 'Title', 'Directed by', 'Written by', 'Original air date', 'Production code', 'U.S. viewers (millions)']\\n Natural Query: What is the number in season of the episode whose production code is pabf05?\\n SQL Query:\"\n",
      "\n",
      "predicted='SELECT 1-28194879-1 WHERE 1-28194879-1.P.S. Production code = pabf05\\n Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected Expected expected Expected expected\\n Expected Expected Expected Expected expected Expected expected\\n Expected Expected Expected expected\\n Expected Expected Expected expected\\n Expected'\n",
      "\n",
      "expected='SELECT # FROM 1-28194879-1 WHERE Production code = PABF05</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 1-2679061-6\\n Columns: ['Pick #', 'Player', 'Position', 'Nationality', 'NHL team', 'College/junior/club team']\\n Natural Query: What is the highest pick number for player don barber?\\n SQL Query:\"\n",
      "\n",
      "predicted=\"SELECT Pick # from 1-2679061-6 WHERE Player = 'Don'\\n\\nThe SQL query will return the highest pick number for player no. 1, which is 12679061.\\n\\nThe SQL query will return the highest pick number for player no. 1, which is 12679061.\\n\\nThe SQL query will return the highest pick number for player no. 1, which is 12679061.\\n\\nThe SQL query will return the highest pick number for player no\"\n",
      "\n",
      "expected='SELECT MAX Pick # FROM 1-2679061-6 WHERE Player = Don Barber</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 1-20704243-6\\n Columns: ['Series #', 'Season #', 'Title', 'Directed by', 'Written by', 'Original air date', 'U.S. viewers (in millions)']\\n Natural Query: What was the original air date for season 11?\\n SQL Query:\"\n",
      "\n",
      "predicted=\"SELECT Original air date FROM 1-20704243-6 WHERE Series # = 11 AND Season # = 11 AND Title = 'The Star Wars Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series Series\"\n",
      "\n",
      "expected='SELECT Original air date FROM 1-20704243-6 WHERE Season # = 11</s>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Table: 2-10808089-4\\n Columns: ['Home team', 'Home team score', 'Away team', 'Away team score', 'Venue', 'Crowd', 'Date']\\n Natural Query: Which home team scored 9.13 (67)?\\n SQL Query:\"\n",
      "\n",
      "predicted='SELECT 1.13 FROM 2-10808089-4 WHERE 9.13\\n Expected Output: 1.13\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None\\n\\nExpected Output: None'\n",
      "\n",
      "expected='SELECT Home team FROM 2-10808089-4 WHERE Home team score = 9.13 (67)</s>'\n",
      "text=\"Table: 1-17355743-1\\n Columns: ['Series #', 'Season #', 'Title', 'Directed by', 'Written by', 'Original air date']\\n Natural Query: Name who directed the episode by joe sachs and david zabel\\n SQL Query:\"\n",
      "\n",
      "predicted='SELECT Director, Title, Original air date from 1-173'\n",
      "\n",
      "expected='SELECT Directed by FROM 1-17355743-1 WHERE Written by = Joe Sachs and David Zabel</s>'\n"
     ]
    }
   ],
   "source": [
    "split = \"test\"\n",
    "length = len(dataset[split])\n",
    "for i in range(10):\n",
    "    index = random.randint(0,length)\n",
    "    text = f'{dataset[split][index][\"content\"].split(\"SQL Query:\")[0]}SQL Query:'\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")#, add_special_tokens=False)\n",
    "    inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
    "    with torch.autocast(dtype=torch.bfloat16, device_type=\"cuda\"):\n",
    "        outputs = model.generate(**inputs, \n",
    "                                 max_new_tokens=128, \n",
    "                                 eos_token_id=tokenizer.eos_token_id)\n",
    "    predicted = tokenizer.decode(outputs[0]).split(\"SQL Query:\")[-1].strip()\n",
    "    expected = dataset[split][index][\"content\"].split(\"SQL Query:\")[-1].strip()\n",
    "    \n",
    "    print(f\"{text=}\\n\\n{predicted=}\\n\\n{expected=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc7b18b2-f718-43b4-aad8-baa4e1d2dab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 07:15:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   42C    P2             62W /  300W |   46156MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb52fa-e4f8-4a6f-8f0f-bf7edf59aaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
